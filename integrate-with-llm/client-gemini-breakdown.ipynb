{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Gemini Client - Interactive Breakdown\n",
    "\n",
    "This notebook breaks down the `client-gemini.py` into interactive components for step-by-step execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from contextlib import AsyncExitStack\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Allow nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API Key loaded: AIzaSyAkFnPWLleFVjIP...\n",
      "âœ… Model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Get API key and model from environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"âŒ GEMINI_API_KEY is not set in .env file\")\n",
    "\n",
    "print(f\"âœ… API Key loaded: {api_key[:20]}...\")\n",
    "print(f\"âœ… Model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemini client initialized\n",
      "   Using model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Create Gemini client\n",
    "gemini_client = genai.Client(api_key=api_key)\n",
    "\n",
    "print(\"âœ… Gemini client initialized\")\n",
    "print(f\"   Using model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Connect to MCP Server\n",
    "\n",
    "This connects to the MCP server running `server.py` via stdio transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Connected to MCP server with tools:\n",
      "  - get_knowledge_base: Retrieve the knowledge base containing sarcastic/funny responses to common workp...\n"
     ]
    }
   ],
   "source": [
    "# Initialize session variables\n",
    "session: Optional[ClientSession] = None\n",
    "exit_stack = AsyncExitStack()\n",
    "stdio: Optional[Any] = None\n",
    "write: Optional[Any] = None\n",
    "\n",
    "async def connect_to_server(server_script_path: str = \"server.py\"):\n",
    "    global session, stdio, write, exit_stack\n",
    "    \n",
    "    # Configure server parameters\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[server_script_path],\n",
    "    )\n",
    "    \n",
    "    # Connect to server via stdio\n",
    "    stdio_transport = await exit_stack.enter_async_context(\n",
    "        stdio_client(server_params)\n",
    "    )\n",
    "    stdio, write = stdio_transport\n",
    "    \n",
    "    # Create and initialize session\n",
    "    session = await exit_stack.enter_async_context(\n",
    "        ClientSession(stdio, write)\n",
    "    )\n",
    "    \n",
    "    await session.initialize()\n",
    "    \n",
    "    # List available tools\n",
    "    tools_result = await session.list_tools()\n",
    "    print(\"\\nâœ… Connected to MCP server with tools:\")\n",
    "    for tool in tools_result.tools:\n",
    "        print(f\"  - {tool.name}: {tool.description[:80]}...\")\n",
    "    \n",
    "    return tools_result\n",
    "\n",
    "# Run the connection\n",
    "tools_result = await connect_to_server(\"server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert MCP Tools to Gemini Format\n",
    "\n",
    "Convert the MCP tools into Gemini's function declaration format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted 1 tools to Gemini format\n",
      "  - get_knowledge_base\n"
     ]
    }
   ],
   "source": [
    "async def get_gemini_tools() -> List[types.Tool]:\n",
    "    \"\"\"Convert MCP tools to Gemini format\"\"\"\n",
    "    tools_result = await session.list_tools()\n",
    "    \n",
    "    function_decls = [\n",
    "        types.FunctionDeclaration(\n",
    "            name=tool.name,\n",
    "            description=tool.description,\n",
    "            parameters=tool.inputSchema,\n",
    "        )\n",
    "        for tool in tools_result.tools\n",
    "    ]\n",
    "    \n",
    "    return [types.Tool(function_declarations=function_decls)]\n",
    "\n",
    "# Get tools in Gemini format\n",
    "gemini_tools = await get_gemini_tools()\n",
    "print(f\"âœ… Converted {len(gemini_tools[0].function_declarations)} tools to Gemini format\")\n",
    "for func_decl in gemini_tools[0].function_declarations:\n",
    "    print(f\"  - {func_decl.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Send Query to Gemini (First Call)\n",
    "\n",
    "Send a query to Gemini with the available tools. Gemini will decide whether to call the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¤ Query: Hey, how are you doing?\n",
      "\n",
      "âœ… Received response from Gemini\n",
      "\n",
      "Response parts: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "query = \"Hey, how are you doing?\"\n",
    "print(f\"\\nðŸ“¤ Query: {query}\\n\")\n",
    "\n",
    "# Define system instruction\n",
    "system_instruction = (\n",
    "    \"You are a workplace assistant with access to a knowledge base of sarcastic/funny responses. \"\n",
    "    \"When the user asks a question, ALWAYS check the knowledge base first using the available tool \"\n",
    "    \"to see if there's a matching response. If found, use that response.\"\n",
    ")\n",
    "\n",
    "# Call Gemini with tools\n",
    "response = gemini_client.models.generate_content(\n",
    "    model=model,\n",
    "    contents=query,\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=gemini_tools,\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"âœ… Received response from Gemini\")\n",
    "print(f\"\\nResponse parts: {len(response.candidates[0].content.parts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            ),\n",
       "            thought_signature=b'\\n\\x9c\\x02\\x01r\\xc8\\xda|\\xd8\\x85~\\t\\x8a\\x18\\xbd:\\xea\\xac\\xe3\\xd2\\x05q\\xb9\\xe8\\x06\\xbf:DCyIb\\xb5\\xa3\\x05\\x16)\\xd4\\xb4U\\xc9a\\n[\\xaeY\\xdb\\x1e\\x83\\x90\\x1e#\\xa5\\x85\\x80N\\x00\\x94\\xb35\\xd4\\xa7q\\xbd\\x060\\x0b,\\xd3\\x8f\\x08>0\\x8b\\xdb\\x10\\xda\\x85\\xabE\\x81SiMA>\\x96#Gh m\\x80\\xca\\xd7\\xde\\xf5...'\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='1nldafKSD_6_juMPxuza8Qk',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=12,\n",
       "    prompt_token_count=147,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=147\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=55,\n",
       "    total_token_count=214\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Check if Tool Was Called\n",
    "\n",
    "Inspect the response to see if Gemini decided to call any tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={},\n",
      "    name='get_knowledge_base'\n",
      "  ),\n",
      "  thought_signature=b'\\n\\x9c\\x02\\x01r\\xc8\\xda|\\xd8\\x85~\\t\\x8a\\x18\\xbd:\\xea\\xac\\xe3\\xd2\\x05q\\xb9\\xe8\\x06\\xbf:DCyIb\\xb5\\xa3\\x05\\x16)\\xd4\\xb4U\\xc9a\\n[\\xaeY\\xdb\\x1e\\x83\\x90\\x1e#\\xa5\\x85\\x80N\\x00\\x94\\xb35\\xd4\\xa7q\\xbd\\x060\\x0b,\\xd3\\x8f\\x08>0\\x8b\\xdb\\x10\\xda\\x85\\xabE\\x81SiMA>\\x96#Gh m\\x80\\xca\\xd7\\xde\\xf5...'\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "parts = response.candidates[0].content.parts\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part 1:\n",
      "  âœ… Function Call Detected!\n",
      "  Function name: get_knowledge_base\n",
      "  Arguments: {}\n",
      "\n",
      "âœ… Gemini decided to call a tool!\n"
     ]
    }
   ],
   "source": [
    "parts = response.candidates[0].content.parts\n",
    "tool_called = False\n",
    "\n",
    "for i, part in enumerate(parts):\n",
    "    print(f\"\\nPart {i+1}:\")\n",
    "    if part.function_call:\n",
    "        tool_called = True\n",
    "        print(f\"  âœ… Function Call Detected!\")\n",
    "        print(f\"  Function name: {part.function_call.name}\")\n",
    "        print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "    elif part.text:\n",
    "        print(f\"  Text response: {part.text}\")\n",
    "\n",
    "if tool_called:\n",
    "    print(\"\\nâœ… Gemini decided to call a tool!\")\n",
    "else:\n",
    "    print(\"\\nâŒ No tool was called. Gemini responded directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Execute Tool Call via MCP\n",
    "\n",
    "If a tool was called, execute it through the MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Executing tool: get_knowledge_base\n",
      "   Arguments: {}\n",
      "\n",
      "âœ… Tool result received:\n",
      "   Here is the retrieved knowledge base:\n",
      "\n",
      "Q1: Hey, how are you doing?\n",
      "A1: ðŸ‘\n",
      "\n",
      "Q2: Do you have a minute? I have some urgent work.\n",
      "A2: ummmmmmmmmmm no\n",
      "\n",
      "Q3: How is this session going?\n",
      "A3: thinking to resign\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tool_parts = []\n",
    "\n",
    "if tool_called:\n",
    "    for part in parts:\n",
    "        if part.function_call:\n",
    "            fn_name = part.function_call.name\n",
    "            args = dict(part.function_call.args)\n",
    "            \n",
    "            print(f\"ðŸ”§ Executing tool: {fn_name}\")\n",
    "            print(f\"   Arguments: {args}\")\n",
    "            \n",
    "            # Call the MCP tool\n",
    "            result = await session.call_tool(fn_name, arguments=args)\n",
    "            \n",
    "            print(f\"\\nâœ… Tool result received:\")\n",
    "            print(f\"   {result.content[0].text[:200]}...\\n\")\n",
    "            \n",
    "            # Prepare tool response for Gemini\n",
    "            tool_parts.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=fn_name,\n",
    "                    response={\"result\": result.content[0].text},\n",
    "                )\n",
    "            )\n",
    "else:\n",
    "    print(\"No tools to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Send Tool Result Back to Gemini (Follow-up Call)\n",
    "\n",
    "Send the tool results back to Gemini to get the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tool_called:\n",
    "    print(\"ðŸ“¤ Sending tool results back to Gemini...\\n\")\n",
    "    \n",
    "    # Send follow-up with tool results\n",
    "    followup = gemini_client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=[\n",
    "            query,\n",
    "            response.candidates[0].content,\n",
    "            types.Content(role=\"tool\", parts=tool_parts),\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    final_response = followup.text\n",
    "else:\n",
    "    final_response = response.text\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“¥ FINAL RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{final_response}\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test with Different Queries\n",
    "\n",
    "Try different queries from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_query(query: str) -> str:\n",
    "    \"\"\"Process a query end-to-end\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get tools\n",
    "    tools = await get_gemini_tools()\n",
    "    \n",
    "    # First call to Gemini\n",
    "    response = gemini_client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=query,\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=tools,\n",
    "            system_instruction=system_instruction,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Check for tool calls\n",
    "    parts = response.candidates[0].content.parts\n",
    "    tool_parts = []\n",
    "    tool_called = False\n",
    "    \n",
    "    for part in parts:\n",
    "        if part.function_call:\n",
    "            tool_called = True\n",
    "            fn_name = part.function_call.name\n",
    "            args = dict(part.function_call.args)\n",
    "            \n",
    "            # Execute tool\n",
    "            result = await session.call_tool(fn_name, arguments=args)\n",
    "            \n",
    "            tool_parts.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=fn_name,\n",
    "                    response={\"result\": result.content[0].text},\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Get final response\n",
    "    if not tool_called:\n",
    "        return response.text\n",
    "    \n",
    "    followup = gemini_client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=[\n",
    "            query,\n",
    "            response.candidates[0].content,\n",
    "            types.Content(role=\"tool\", parts=tool_parts),\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return followup.text\n",
    "\n",
    "# Test different queries\n",
    "test_queries = [\n",
    "    \"Any updates?\",\n",
    "    \"Do you have a minute?\",\n",
    "    \"How is this session going?\",\n",
    "    \"Why are you so quiet?\"\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    result = await process_query(test_query)\n",
    "    print(f\"Response: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Cleanup\n",
    "\n",
    "Close the MCP session and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "await exit_stack.aclose()\n",
    "print(\"âœ… Cleaned up MCP session\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
